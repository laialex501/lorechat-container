# LoreChat ğŸ¤–

A modern AI chat platform that makes website content come alive through conversation.

> ğŸ“š For development guides, see [DEVELOPMENT.md](DEVELOPMENT.md)

## Project Overview ğŸš€

Welcome to LoreChat! This is my GenAI portfolio project to demonstrate graph-based AI conversations. 

What makes it special? LoreChat uses LangGraph to create smart, flowing conversations about your website content. Think of it as your website's friendly AI guide.

Why LangGraph? I found that traditional chat systems can get messy with complex conversations. LangGraph keeps things clean and organized. It's like having a well-designed roadmap for each chat.

Want to see how it works? LoreChat aims to handle about 50 users at once, with a quick response time. The magic happens through:
- Smart context finding ğŸ”
- Natural conversations ğŸ’­
- Clear source tracking ğŸ“

LoreChat works with [LoreChatCDK](https://github.com/laialex501/lorechat-cdk) to run smoothly in the cloud.

### Key Features âš¡

Smart Conversations:
- Graph-based chat flows ğŸ—ºï¸
- Smart memory management ğŸ§ 
- Source tracking in responses ğŸ“š
- Fast responses âš¡

Tech Choices:
- Works with OpenAI and Claude ğŸ¤–
- Local testing with FAISS ğŸ”¬
- Cloud ready with Upstash Vector â˜ï¸
- Full system monitoring ğŸ“Š

### Tech Stack ğŸ› ï¸

Core Tools:
- Python 3.9 ğŸ
- Streamlit ğŸ“±
- LangChain & LangGraph ğŸ”—
- FAISS & Upstash Vector ğŸ”

Support Tools:
- AWS SDK â˜ï¸
- Docker/Finch ğŸ‹
- pytest ğŸ§ª

## System Architecture ğŸ—ï¸

Let's look at how LoreChat works. I designed it to be clear and organized:

```mermaid
graph TD
    subgraph "Frontend Layer"
        A[Streamlit UI] --> B[Chat Service]
        B --> C[Session State]
    end

    subgraph "LangGraph Workflow"
        D[Graph Manager] --> E[Retrieve Node]
        D --> F[Respond Node]
        G[State Manager] --> D
    end

    subgraph "Core Services"
        H[LLM Factory] --> I[Provider Services]
        J[Vector Store] --> K[Hybrid Search]
        L[Prompt Factory] --> M[Persona System]
    end

    B --> D
    E --> J
    F --> H
    F --> L
```

### Message Lifecycle ğŸ”„

Here's what happens when you chat:

```mermaid
sequenceDiagram
    participant U as User
    participant S as Streamlit
    participant G as Graph
    participant V as Vector Store
    participant L as LLM

    U->>S: Send Message
    S->>G: Process Message
    G->>V: Retrieve Context
    V-->>G: Return Documents
    G->>L: Generate Response
    L-->>G: Return Response
    G->>S: Stream Response
    S->>U: Display Response
```

## Core Parts ğŸ”§

### 1. LangGraph Implementation ğŸ§ 

I picked LangGraph because it's great at keeping conversations organized. Here's how it works:

```mermaid
graph LR
    A[User Input] --> B[Retrieve Node]
    B --> C[Respond Node]
    
    subgraph "State Management"
        D[Messages]
        E[Retrieved Docs]
        F[Thread ID]
    end
    
    B --> E
    C --> D
```

The graph consists of two main nodes:
1. **Retrieve Node**: Handles context retrieval using hybrid search ğŸ”
2. **Respond Node**: Generates responses with source attribution ğŸ’¬

Why this works well:
- Keeps track of everything clearly ğŸ“
- Shows exactly what's happening ğŸ”„
- Easy to test and watch ğŸ”¬
- Simple to add new features âœ¨

### 2. Vector Store Architecture ğŸ”

I implemented a hybrid search approach using Upstash Vector:

```mermaid
graph TD
    A[Query] --> B[Create Embeddings]
    B --> C[Dense Vector]
    B --> D[Sparse Vector]
    
    C --> E[Hybrid Search]
    D --> E
    
    E --> F[RRF Fusion]
    F --> G[Results]
```

The hybrid search combines:
- Dense vectors for semantic similarity ğŸ§®
- Sparse vectors for keyword matching ğŸ”¤
- Reciprocal Rank Fusion (RRF) for result combination ğŸ”€

This helps find the most relevant information every time.

### 3. Memory System ğŸ§ 

Each chat has its own memory thread:

```python
class ChatState(MessagesState):
    """Keeps track of chat context."""
    messages: List[BaseMessage]
    retrieved_docs: Optional[List[Document]]
```

This helps by:
- Keeping chats organized ğŸ“‹
- Preventing mistakes âœ…
- Saving progress ğŸ’¾
- Easy backup ğŸ”„

### 4. LLM Integration ğŸ¤–

The LLM service uses a factory pattern for provider flexibility:

```mermaid
graph TD
    A[LLM Factory] --> B[Base LLM Service]
    B --> C[OpenAI Service]
    B --> D[Bedrock Service]
    
    subgraph "Features"
        E[Streaming]
        F[Error Handling]
        G[Rate Limiting]
    end
    
    C --> E
    C --> F
    C --> G
    D --> E
    D --> F
    D --> G
```

This lets us:
- Switch AI models easily ğŸ”„
- Keep things consistent ğŸ“‹
- Handle problems smoothly ğŸ› ï¸
- Use resources wisely âš¡

## Technical Choices ğŸ¤”

### 1. Why LangGraph? 

I picked LangGraph over regular chains because:
- Better memory handling ğŸ§ 
- Explicit workflow definition ğŸ—ºï¸
- Easier testing ğŸ”¬
- Simple upgrades â¬†ï¸

It takes more work at first, but makes everything easier later.

### 2. Why Upstash Vector? 

After trying many options, I chose Upstash Vector because:
- Hybrid search capabilities ğŸ”
- Easy to use ğŸ¯
- Cost friendly ğŸ’°
- Great for developers ğŸ‘©â€ğŸ’»

The abstraction layer makes switching to a different vendor possible if needed.

### 3. Chat Memory Design 

Using thread IDs for session management provides:
- Clear conversation boundaries ğŸ—‚ï¸
- Simple state persistence ğŸ’¾
- Easy scaling ğŸ“ˆ
- Recovery capabilities ğŸ”§

It uses more memory, but it's manageable with proper cleanup.

## Making Things Fast âš¡

1. **Hybrid Search**
   - Sparse vector creation with threshold filtering ğŸ”
   - Reciprocal Rank Fusion for result combination ğŸ”€
   - Metadata-enhanced retrieval ğŸ“

2. **Response Streaming**
   - Chunked response delivery ğŸŒŠ
   - Progressive UI updates ğŸ“±
   - Efficient memory use ğŸ’¾

3. **State Management**
   - Selective state persistence ğŸ’¾
   - Efficient checkpointing âœ…
   - Memory-aware cleanup ğŸ§¹

## Future Plans ğŸ”®

While the current system is robust, I'm considering several enhancements:

1. **Advanced Graph Features**
   - Multi-step reasoning nodes ğŸ§ 
   - Dynamic node selection ğŸ”„
   - Parallel processing âš¡

2. **Vector Store Optimizations**
   - Vector store caching ğŸ’¾
   - Progressive indexing ğŸš€
   - Automatic reindexing ğŸ”„

3. **UI Enhancements**
   - Real-time typing indicators âŒ¨ï¸
   - Better sources ğŸ“š
   - Interactive exploring ğŸ”

4. **System Updates**
   - Distributed state management ğŸ¤
   - Enhanced error recovery ğŸ”§
   - Advanced monitoring ğŸ‘€

## License ğŸ“œ

This project is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.
